name: a-team

on:
  pull_request:
    paths-ignore:
      - '**.md'
    branches:
      - main

jobs:
  infra_provisioning:
    # Workflow can be invoked only when triggered from base repo. GitHub is
    # preventing from sharing secrets with workflows invoked from forks
    if: github.event.pull_request.head.repo.full_name == github.repository
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.ATEAM_AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.ATEAM_AWS_SECRET_ACCESS_KEY }}
      TF_VAR_REPO_BUCKET: ${{ secrets.AWS_BUCKET_REPOS }}
      TF_VAR_IMAGE_BUCKET: ${{ secrets.AWS_BUCKET_IMAGES }}

    name: setup infrastructure
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Terraform Setup
        uses: hashicorp/setup-terraform@v1
        with:
          terraform_version: 1.0.2

      - name: Terraform Format
        id: fmt
        run: terraform -chdir=tform/infra/ fmt -check

      - name: Terraform Init
        id: init
        run: terraform -chdir=tform/infra/ init
        continue-on-error: false

      - name: Terraform Validate
        id: validate
        run: terraform -chdir=tform/infra/ validate -no-color

      - name: Terraform Plan
        id: plan
        run: terraform -chdir=tform/infra/ plan -no-color -var="repo_bucket=${TF_VAR_REPO_BUCKET}" -var="image_bucket=${TF_VAR_IMAGE_BUCKET}"

      - name: Terraform Apply
        id: apply
        run: terraform -chdir=tform/infra/ apply -auto-approve -var="repo_bucket=$TF_VAR_REPO_BUCKET" -var="image_bucket=TF_VAR_IMAGE_BUCKET"
        continue-on-error: true

      - name: Check Error
        if: ${{ steps.apply.outputs.exitcode != 0 && !contains( steps.apply.outputs.stderr, 'BucketAlreadyOwnedByYou') }}
        run: exit 1

  build:
    if: github.event.pull_request.head.repo.full_name == github.repository

    # We accept only one job for now, ostree.sh cannot do parallel builds
    concurrency: single
    runs-on: ubuntu-latest
    container: quay.io/testing-farm/tmt:1.6.0
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v2

      - name: Create yum repo
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.ATEAM_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.ATEAM_AWS_SECRET_ACCESS_KEY }}
          AWS_BUCKET_REPOS: ${{ secrets.AWS_BUCKET_REPOS }}
          AWS_REGION: "eu-west-1"
        run: |
          sudo dnf install -y createrepo_c python3-pip
          pip3 install -U pip
          pip3 install -r ./ci/create_yum_repo/requirements.txt
          ./ci/create_yum_repo/main.py

      - name: Create SSH key
        run: |
          mkdir -p -m 700 ~/.ssh/
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
        shell: bash

      - name: Image build
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          tmt run -vvv --all --environment="GITHUB_RUN_ID=$GITHUB_RUN_ID" --environment="AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}" --environment="AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}" --environment="GITLAB_PTS_PIPELINE_TOKEN=${{ secrets.GITLAB_PTS_PIPELINE_TOKEN }}" --environment="GITLAB_PTS_API_TOKEN=${{ secrets.GITLAB_PTS_API_TOKEN }}" provision --how connect --guest ${{ secrets.HOST }} --key ~/.ssh/id_rsa
        shell: bash

      - name: Clean and fix
        # This should run even if the job is canceled
        if: ${{ always() }}
        run: |
          scp -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa tests/ci/fix_baremetal.sh ${{ secrets.HOST }}:
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa ${{ secrets.HOST }} ./fix_baremetal.sh
        shell: bash
