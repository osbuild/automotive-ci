name: a-team

on:
  pull_request:
    paths-ignore:
      - '**.md'
    branches:
      - main

jobs:
  infra_provisioning:
    # Workflow can be invoked only when triggered from base repo. GitHub is
    # preventing from sharing secrets with workflows invoked from forks
    if: github.event.pull_request.head.repo.full_name == github.repository
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.ATEAM_AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.ATEAM_AWS_SECRET_ACCESS_KEY }}
      TF_VAR_REPO_BUCKET: ${{ secrets.AWS_BUCKET_REPOS }}
      TF_VAR_IMAGE_BUCKET: ${{ secrets.AWS_BUCKET_IMAGES }}

    name: setup infrastructure
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Terraform Setup
        uses: hashicorp/setup-terraform@v1
        with:
          terraform_version: 1.0.2

      - name: Terraform Format
        id: fmt
        run: terraform -chdir=tform/infra/ fmt -check

      - name: Terraform Init
        id: init
        run: terraform -chdir=tform/infra/ init
        continue-on-error: false

      - name: Terraform Validate
        id: validate
        run: terraform -chdir=tform/infra/ validate -no-color

      - name: Terraform Plan
        id: plan
        run: terraform -chdir=tform/infra/ plan -no-color -var="repo_bucket=${TF_VAR_REPO_BUCKET}" -var="image_bucket=${TF_VAR_IMAGE_BUCKET}"

      - name: Terraform Apply
        id: apply
        run: terraform -chdir=tform/infra/ apply -auto-approve -var="repo_bucket=${TF_VAR_REPO_BUCKET}" -var="image_bucket=${TF_VAR_IMAGE_BUCKET}"
        continue-on-error: true

      - name: Check Error
        if: ${{ steps.apply.outputs.exitcode != 0 && !contains( steps.apply.outputs.stderr, 'BucketAlreadyOwnedByYou') }}
        run: exit 1

  build:
    if: github.event.pull_request.head.repo.full_name == github.repository

    # We accept only one job for now, ostree.sh cannot do parallel builds
    concurrency: single
    runs-on: ubuntu-latest
    container: quay.io/testing-farm/tmt:1.6.0
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v2

      - name: Create yum repo
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.ATEAM_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.ATEAM_AWS_SECRET_ACCESS_KEY }}
          AWS_BUCKET_REPOS: ${{ secrets.AWS_BUCKET_REPOS }}
          AWS_REGION: "eu-west-1"
        run: |
          sudo dnf install -y createrepo_c python3-pip
          pip3 install -U pip
          pip3 install -r ./ci/create_yum_repo/requirements.txt
          ./ci/create_yum_repo/main.py

      - name: Create SSH key
        run: |
          mkdir -p -m 700 ~/.ssh/
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
        shell: bash

      - name: Create a OSTree commit
        env:
          ENDPOINT: "https://api.dev.testing-farm.io/v0.1/requests"
          API_KEY: ${{ secrets.TF_API_KEY }}
          ARCH: "x86_64"
        run: |
          dnf install -y --repo fedora jq
          cat <<EOF > request.json
          {
            "api_key": "${API_KEY}",
            "test": {
              "fmf": {
                "url": "https://github.com/osbuild/automotive-ci",
                "ref": "${GITHUB_REF}",
                "name": "/tests/ci/create-commit"
              }
            },
            "environments": [
              {
                "arch": "${ARCH}",
                "os": {"compose": "CentOS-Stream-8"},
                "variables": {
                  "UUID": "$GITHUB_SHA",
                  "GITHUB_RUN_ID": "${GITHUB_RUN_ID}"
                }
              }
            ]
          }
          EOF
          curl --silent ${ENDPOINT} \
               --header "Content-Type: application/json" \
               --data @request.json \
               --output response.json
          jq . response.json
          ID=$(jq -r '.id' response.json)
          echo "Wait until the job finished at the Testing Farm"
          while true; do
            rm -f response.json
            curl --silent --output response.json "${ENDPOINT}/${ID}"
            STATUS=$(jq -r '.state' response.json)
            if [[ "$STATUS" == "complete" ]] || [[ "$STATUS" == "error" ]]; then
              echo ; echo "Finished"
              break
            fi
            echo -n "."
            sleep 30
          done
          RESULT=$(jq -r '.result.overall' response.json)
          echo "Result: $RESULT"
          # If the result is an error, there is no report to show
          if [[ "$RESULT" == "error" ]]; then
            jq -r '.result.summary' response.json
            exit 1
          fi
          EXIT_CODE=1
          if [[ "$RESULT" == "passed" ]]; then
            EXIT_CODE=0
          fi
          URL=$(jq -r '.run.artifacts' response.json)
          curl --silent --output report.html "$URL/"
          echo "The build results are here: $URL"
          exit $EXIT_CODE
        shell: bash

      - name: Install VM
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          tmt run -vvv --all \
                  --environment="UUID=$GITHUB_SHA" \
                  --environment="GITHUB_RUN_ID=$GITHUB_RUN_ID" \
                  --environment="AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}" \
                  --environment="AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
                  plan --name /tests/ci/create-vm \
                  provision --how connect --guest ${{ secrets.HOST }} --key ~/.ssh/id_rsa
        shell: bash

      - name: Clean and fix
        # This should run even if the job is canceled
        if: ${{ always() }}
        run: |
          scp -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa tests/ci/fix_baremetal.sh ${{ secrets.HOST }}:
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa ${{ secrets.HOST }} ./fix_baremetal.sh
        shell: bash
